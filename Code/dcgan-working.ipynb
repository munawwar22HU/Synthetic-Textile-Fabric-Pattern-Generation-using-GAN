{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import make_grid\n\ndef show_tensor_images(title, image_tensor, num_images=16, size=(3, 64, 64), nrow=3):\n    plt.figure(figsize = (20,10))\n    image_tensor = (image_tensor + 1) / 2\n    image_unflat = image_tensor.detach().cpu().clamp_(0, 1)\n    image_grid = make_grid(image_unflat[:num_images], nrow=nrow, padding=0)\n    plt.title(title)\n    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n    plt.axis('off')\n    plt.show()\n\nclass Generator(nn.Module):\n    def __init__(self, z_dim=64, im_chan=3, hidden_dim=16):\n        super(Generator, self).__init__()\n        self.z_dim = z_dim\n        # Build the neural network\n        self.dense1 = nn.Linear(z_dim, z_dim)\n        self.dense2 = nn.Linear(z_dim, 64)\n        self.activation = nn.Tanh()\n        self.gen = nn.Sequential(\n            self.make_gen_block(1, hidden_dim * 2),\n            self.make_gen_block(hidden_dim * 2, hidden_dim * 2),\n            self.make_gen_block(hidden_dim * 2, hidden_dim),\n            self.make_gen_block(hidden_dim, hidden_dim),\n            self.make_gen_block(hidden_dim, im_chan, final_layer=True),\n        )\n\n    def make_gen_block(self, input_channels, output_channels, scale_factor=2, kernel_size=4, stride=1, final_layer=False):\n        if not final_layer:\n            return nn.Sequential(\n                nn.Upsample(scale_factor=scale_factor, mode='bilinear', align_corners=False),\n                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n                nn.BatchNorm2d(output_channels),\n                nn.LeakyReLU(negative_slope=0.2)\n            )\n        else:\n            return nn.Sequential(\n                nn.Upsample(scale_factor=scale_factor, mode='bilinear', align_corners=False),\n                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n                nn.LeakyReLU(negative_slope=0.2)\n            )\n\n    def forward(self, noise):\n        x = self.dense1(noise)\n        x = self.activation(x)\n        x = self.dense2(x)\n        x = self.activation(x)\n        x = x.view(-1, 1, 8, 8)\n        # x = noise.view(len(noise), self.z_dim, 1, 1)\n        return self.gen(x)\n\ndef get_noise(n_samples, z_dim, device='cpu'):\n    return torch.randn(n_samples, z_dim, device=device)\n\nclass Critic(nn.Module):\n    def __init__(self, im_chan=1, hidden_dim=16, z_dim=64):\n        super(Critic, self).__init__()\n        self.dense1 = nn.Linear(64, z_dim)\n        self.dense2 = nn.Linear(z_dim, 1)\n        self.activation = nn.Tanh()\n        self.sigmoid = nn.Sigmoid()\n        self.crit = nn.Sequential(\n            self.make_crit_block(im_chan, hidden_dim),\n            self.make_crit_block(hidden_dim, hidden_dim * 2),\n            self.make_crit_block(hidden_dim * 2, hidden_dim * 2),\n            self.make_crit_block(hidden_dim * 2, 1, final_layer=True),\n        )\n\n    def make_crit_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n        if not final_layer:\n            return nn.Sequential(\n                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n                nn.BatchNorm2d(output_channels),\n                nn.LeakyReLU(negative_slope=0.2)\n            )\n        else:\n            return nn.Sequential(\n                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n            )\n\n    def forward(self, image):\n        crit_pred = self.crit(image)\n        x = crit_pred.view(len(crit_pred), -1)\n        x = self.dense1(x)\n        x = self.activation(x)\n        x = self.dense2(x)\n        return self.sigmoid(x)\n    \ndevice = 'cuda'\nim_chan = 3\nz_dim = 64\n\ngen = Generator(z_dim=z_dim, im_chan=im_chan, hidden_dim=48).to(device)\ncrit = Critic(im_chan=im_chan, hidden_dim=48).to(device)\n\nnoise = torch.randn(16, z_dim, device=device)\nfake = gen(noise)\nprint(fake.shape)\ncrit(fake)\nim_size = fake.shape[3]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-04T10:06:55.454605Z","iopub.execute_input":"2021-12-04T10:06:55.454891Z","iopub.status.idle":"2021-12-04T10:06:55.497812Z","shell.execute_reply.started":"2021-12-04T10:06:55.454857Z","shell.execute_reply":"2021-12-04T10:06:55.497075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: get_gradient\ndef get_gradient(crit, real, fake, epsilon):\n    # Mix the images together\n    mixed_images = real * epsilon + fake * (1 - epsilon)\n\n    # Calculate the critic's scores on the mixed images\n    mixed_scores = crit(mixed_images)\n    \n    # Take the gradient of the scores with respect to the images\n    gradient = torch.autograd.grad(\n        # Note: You need to take the gradient of outputs with respect to inputs.\n        # This documentation may be useful, but it should not be necessary:\n        # https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad\n        #### START CODE HERE ####\n        inputs=mixed_images,\n        outputs=mixed_scores,\n        #### END CODE HERE ####\n        # These other parameters have to do with the pytorch autograd engine works\n        grad_outputs=torch.ones_like(mixed_scores), \n        create_graph=True,\n        retain_graph=True,\n    )[0]\n    return gradient\n\n# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: gradient_penalty\ndef gradient_penalty(gradient):\n    # Flatten the gradients so that each row captures one image\n    gradient = gradient.view(len(gradient), -1)\n\n    # Calculate the magnitude of every row\n    gradient_norm = gradient.norm(2, dim=1)\n    \n    # Penalize the mean squared distance of the gradient norms from 1\n    #### START CODE HERE ####\n    penalty = nn.functional.mse_loss(gradient_norm, torch.ones_like(gradient_norm))\n    #### END CODE HERE ####\n    return penalty\n\n# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: get_gen_loss\ndef get_gen_loss(crit_fake_pred):\n    #### START CODE HERE ####\n    gen_loss = -torch.mean(crit_fake_pred)\n    #### END CODE HERE ####\n    return gen_loss\n\n# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: get_crit_loss\ndef get_crit_loss(crit_fake_pred, crit_real_pred, gp, c_lambda):\n    #### START CODE HERE ####\n    mean_fake = torch.mean(crit_fake_pred)\n    mean_real = torch.mean(crit_real_pred)\n    crit_loss = mean_fake - mean_real + gp * c_lambda\n    #### END CODE HERE ####\n    return crit_loss","metadata":{"execution":{"iopub.status.busy":"2021-12-04T10:05:58.471835Z","iopub.execute_input":"2021-12-04T10:05:58.472256Z","iopub.status.idle":"2021-12-04T10:05:58.482046Z","shell.execute_reply.started":"2021-12-04T10:05:58.472217Z","shell.execute_reply":"2021-12-04T10:05:58.481106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -f /fabric_dataset\n!pip install gdown\n!apt-get install unrar\n!gdown --id 1iQST_v2PJSSnuybcg61G2tBG6Nq5QK8v\n!mkdir fabric_dataset\n!mkdir fabric_dataset/class\n!mv Data-Omema.rar fabric_dataset/class/Data-Omema.rar\n!cd fabric_dataset/class/ && unrar e Data-Omema.rar\n","metadata":{"execution":{"iopub.status.busy":"2021-12-04T09:54:00.070374Z","iopub.execute_input":"2021-12-04T09:54:00.070663Z","iopub.status.idle":"2021-12-04T09:54:40.968225Z","shell.execute_reply.started":"2021-12-04T09:54:00.070632Z","shell.execute_reply":"2021-12-04T09:54:40.967274Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cd fabric_dataset/class/ && rm `ls | head -n2000`\n!ls fabric_dataset/class/ | wc","metadata":{"execution":{"iopub.status.busy":"2021-12-04T09:56:15.826365Z","iopub.execute_input":"2021-12-04T09:56:15.827023Z","iopub.status.idle":"2021-12-04T09:56:17.309669Z","shell.execute_reply.started":"2021-12-04T09:56:15.826976Z","shell.execute_reply":"2021-12-04T09:56:17.308847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 6\n\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms, datasets\n\ndata_transform = transforms.Compose([\n        transforms.RandomResizedCrop(size=im_size,scale=(1.0, 1.5)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.6002, 0.5573, 0.5342],\n                             std=[0.0825, 0.0801, 0.0796])\n    ])\nfabric_dataset = datasets.ImageFolder(\n    root='./fabric_dataset',\n    transform=data_transform)\n\ndataloader = torch.utils.data.DataLoader(\n    fabric_dataset,\n    batch_size=batch_size, shuffle=True,\n    num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2021-12-04T10:06:04.816533Z","iopub.execute_input":"2021-12-04T10:06:04.816815Z","iopub.status.idle":"2021-12-04T10:06:04.825879Z","shell.execute_reply.started":"2021-12-04T10:06:04.816784Z","shell.execute_reply":"2021-12-04T10:06:04.825113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom torchvision.utils import make_grid\nfrom tqdm import tqdm\n\nlr = 0.0002\nbeta_1 = 0.5\nbeta_2 = 0.999\nc_lambda = 10\n\ngen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))\ncrit_opt = torch.optim.Adam(crit.parameters(), lr=lr, betas=(beta_1, beta_2))\n\ndef get_noise(n_samples, z_dim, device='cpu'):\n    return torch.randn(n_samples, z_dim, device=device)\n\n\nn_epochs = 50\ndisplay_step = int(len(dataloader.dataset)/batch_size)\ncur_step = 0\ngenerator_losses = []\ncritic_losses = []\ncrit_repeats = 1\nfor epoch in range(n_epochs):\n    # Dataloader returns the batches\n    for real, _ in tqdm(dataloader):\n        cur_batch_size = len(real)\n        real = real.to(device)\n\n        mean_iteration_critic_loss = 0.0\n        for _ in range(crit_repeats):\n            ### Update critic ###\n            crit_opt.zero_grad()\n            fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n            fake = gen(fake_noise)\n            crit_fake_pred = crit(fake.detach())\n            crit_real_pred = crit(real)\n\n            epsilon = torch.rand(len(real), 1, 1, 1, device=device, requires_grad=True)\n            gradient = get_gradient(crit, real, fake.detach(), epsilon)\n            gp = gradient_penalty(gradient)\n            crit_loss = get_crit_loss(crit_fake_pred, crit_real_pred, gp, c_lambda)\n\n            # Keep track of the average critic loss in this batch\n            mean_iteration_critic_loss += crit_loss.item() / crit_repeats\n            # Update gradients\n            crit_loss.backward(retain_graph=True)\n            # Update optimizer\n            crit_opt.step()\n        critic_losses += [mean_iteration_critic_loss]\n\n            ### Update generator ###\n        for _ in range(1):\n            gen_opt.zero_grad()\n            fake_noise_2 = get_noise(cur_batch_size, z_dim, device=device)\n            fake_2 = gen(fake_noise_2)\n            crit_fake_pred = crit(fake_2)\n\n            gen_loss = get_gen_loss(crit_fake_pred)\n            gen_loss.backward()\n\n            # Update the weights\n            gen_opt.step()\n            # Keep track of the average generator loss\n        generator_losses += [gen_loss.item()]\n\n        ### Visualization code ###\n        if cur_step % display_step == 0 and cur_step > 0:\n            gen_mean = sum(generator_losses[-display_step:]) / display_step\n            crit_mean = sum(critic_losses[-display_step:]) / display_step\n            print(f\"Step {cur_step}: Generator loss: {gen_mean}, critic loss: {crit_mean}\")\n            \n\n            show_tensor_images('fake',fake)\n            show_tensor_images('real',real)\n            step_bins = 20\n            num_examples = (len(generator_losses) // step_bins) * step_bins\n            plt.plot(\n                range(num_examples // step_bins), \n                torch.Tensor(generator_losses[:num_examples]).view(-1, step_bins).mean(1),\n                label=\"Generator Loss\"\n            )\n            plt.plot(\n                range(num_examples // step_bins), \n                torch.Tensor(critic_losses[:num_examples]).view(-1, step_bins).mean(1),\n                label=\"Critic Loss\"\n            )\n            plt.legend()\n            plt.show()\n\n        cur_step += 1","metadata":{"execution":{"iopub.status.busy":"2021-12-04T10:07:01.394652Z","iopub.execute_input":"2021-12-04T10:07:01.394928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms, datasets\nfrom torch.utils import data\ndataset = datasets.ImageFolder('./fabric_dataset', transform=transforms.Compose([transforms.Resize(64),\n                             transforms.CenterCrop(64),\n                             transforms.ToTensor()]))\n\nloader = data.DataLoader(dataset,\n                         batch_size=10,\n                         num_workers=0,\n                         shuffle=False)\n\nmean = 0.0\nfor images, _ in loader:\n    batch_samples = images.size(0) \n    images = images.view(batch_samples, images.size(1), -1)\n    mean += images.mean(2).sum(0)\nmean = mean / len(loader.dataset)\n\nvar = 0.0\nfor images, _ in loader:\n    batch_samples = images.size(0)\n    images = images.view(batch_samples, images.size(1), -1)\n    var += ((images - mean.unsqueeze(1))**2).sum([0,2])\nstd = torch.sqrt(var / (len(loader.dataset)*224*224))\n\nprint(mean, std)","metadata":{"execution":{"iopub.status.busy":"2021-12-03T19:39:50.131476Z","iopub.execute_input":"2021-12-03T19:39:50.132832Z","iopub.status.idle":"2021-12-03T19:39:58.879466Z","shell.execute_reply.started":"2021-12-03T19:39:50.13278Z","shell.execute_reply":"2021-12-03T19:39:58.878497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(gen.state_dict(), 'gen.pth')\ntorch.save(crit.state_dict(), 'crit.pth')","metadata":{"execution":{"iopub.status.busy":"2021-12-03T20:14:16.433679Z","iopub.execute_input":"2021-12-03T20:14:16.434016Z","iopub.status.idle":"2021-12-03T20:14:16.458868Z","shell.execute_reply.started":"2021-12-03T20:14:16.433983Z","shell.execute_reply":"2021-12-03T20:14:16.457863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gen.load_state_dict(torch.load('gen.pth', map_location=device))\ncrit.load_state_dict(torch.load('crit.pth', map_location=device))","metadata":{"execution":{"iopub.status.busy":"2021-12-03T20:25:30.795997Z","iopub.execute_input":"2021-12-03T20:25:30.796631Z","iopub.status.idle":"2021-12-03T20:25:30.818999Z","shell.execute_reply.started":"2021-12-03T20:25:30.796587Z","shell.execute_reply":"2021-12-03T20:25:30.818285Z"},"trusted":true},"execution_count":null,"outputs":[]}]}